# S3 Backend - S3Backend

## Overview

The `S3Backend` is an implementation of the `StorageBackend` trait allowing file storage on S3-compatible services. It supports all providers respecting the standard S3 API.

## Supported Providers

### Cloudflare R2
- 100% S3-compatible API
- No egress fees
- Native CDN integration
- Excellent global performance

### AWS S3
- Original Amazon S3 service
- Complete integration with AWS ecosystem
- Support for all S3 features
- Multi-region, versioning, lifecycle policies

### MinIO
- Open source self-hosted S3
- On-premise deployment
- Private cloud compatible
- Optimal performance

### DigitalOcean Spaces
- DigitalOcean's S3-compatible service
- Integrated CDN
- Simple and predictable pricing
- Good performance

### Others
Any service implementing the standard S3 API works:
- Wasabi
- Backblaze B2
- Linode Object Storage
- IBM Cloud Object Storage

## Architecture

```mermaid
graph TB
    subgraph S3Backend
        Config[Configuration]
        Client[AWS SDK Client]
        KeyBuilder[Key Builder]
    end

    subgraph AWS_SDK
        Creds[Credentials]
        HTTP[HTTP Client]
        Signer[Request Signer]
    end

    subgraph S3_Service
        API[S3 API Endpoint]
        Bucket[Bucket Storage]
    end

    Config --> Client
    Config --> Creds
    Client --> HTTP
    Creds --> Signer
    Signer --> HTTP
    HTTP --> API
    API --> Bucket
    KeyBuilder --> API
```

## Configuration

### Required Parameters

**endpoint_url** (String)
- S3 endpoint URL
- Cloudflare R2: `https://[account-id].r2.cloudflarestorage.com`
- AWS S3: `https://s3.[region].amazonaws.com`
- MinIO: `http://minio.local:9000`

**region** (String)
- AWS region or equivalent
- Cloudflare R2: `auto`
- AWS S3: `us-east-1`, `eu-west-1`, etc.
- MinIO: Can be arbitrary

**access_key_id** (String)
- Access key identifier
- Generated by S3 provider
- Equivalent to username

**secret_access_key** (String)
- Secret access key
- Keep confidential
- Equivalent to password

**bucket_name** (String)
- S3 bucket name
- Must exist beforehand
- Must be unique (depending on provider)

**public_url** (String)
- Public URL to access files
- Can be a CDN or direct bucket URL
- Cloudflare R2: `https://cdn.example.com`
- AWS S3: `https://bucket.s3.amazonaws.com`

### Optional Parameters

**bucket_prefix** (String)
- Prefix added to all keys
- Allows file organization in bucket
- Example: `minecraft/servers/`
- Default: empty

## Operations

### Initialization

```mermaid
sequenceDiagram
    participant App
    participant S3B as S3Backend
    participant SDK as AWS SDK
    participant S3 as S3 Service

    App->>S3B: new(config)
    S3B->>SDK: Create Credentials
    Note over SDK: access_key_id + secret_access_key

    S3B->>SDK: defaults(BehaviorVersion::latest())
    S3B->>SDK: credentials_provider(creds)
    S3B->>SDK: region(region)
    S3B->>SDK: endpoint_url(endpoint)
    S3B->>SDK: load().await

    Note over SDK: Configuration loaded

    S3B->>SDK: Client::new(config)
    SDK-->>S3B: S3 Client ready

    S3B->>S3B: Store client, bucket, url, prefix
    S3B-->>App: S3Backend instance
```

### File Upload

```mermaid
sequenceDiagram
    participant Caller
    participant S3B as S3Backend
    participant FS as File System
    participant SDK as AWS SDK
    participant S3 as S3 Service

    Caller->>S3B: upload_file(local_path, remote_key)

    S3B->>S3B: build_key(remote_key)
    Note over S3B: Add bucket_prefix if configured

    S3B->>FS: tokio::fs::read(local_path)
    FS-->>S3B: Vec<u8> file data

    S3B->>S3B: ByteStream::from(file_data)

    S3B->>SDK: put_object()
    Note over SDK: .bucket(bucket_name)<br/>.key(full_key)<br/>.body(byte_stream)

    SDK->>S3: HTTP PUT /bucket/key
    Note over SDK,S3: Headers: Authorization, Content-Length, etc.

    alt Upload Success
        S3-->>SDK: 200 OK
        SDK-->>S3B: Ok()
        S3B->>S3B: get_url(remote_key)
        S3B-->>Caller: Ok(public_url)
    else Upload Error
        S3-->>SDK: Error (403, 404, 500, etc.)
        SDK-->>S3B: SdkError
        S3B->>S3B: Map to UploadError
        S3B-->>Caller: Err(UploadError(key, msg))
    end
```

**Implementation Details:**

1. **File Reading**: Uses `tokio::fs::read` for complete asynchronous reading
2. **ByteStream**: Conversion to ByteStream for AWS SDK
3. **put_object**: Standard S3 operation with necessary metadata
4. **URL generation**: Construction of public URL for uploaded file

### File Deletion

```mermaid
sequenceDiagram
    participant Caller
    participant S3B as S3Backend
    participant SDK as AWS SDK
    participant S3 as S3 Service

    Caller->>S3B: delete_file(remote_key)

    S3B->>S3B: build_key(remote_key)
    Note over S3B: Add bucket_prefix

    S3B->>SDK: delete_object()
    Note over SDK: .bucket(bucket_name)<br/>.key(full_key)

    SDK->>S3: HTTP DELETE /bucket/key

    alt Delete Success
        S3-->>SDK: 204 No Content
        SDK-->>S3B: Ok()
        S3B-->>Caller: Ok(())
    else Delete Error
        S3-->>SDK: Error
        SDK-->>S3B: SdkError
        S3B->>S3B: Map to DeleteError
        S3B-->>Caller: Err(DeleteError(key, msg))
    end
```

**Idempotence:**
Deleting an already deleted file generally returns success (204). The SDK handles this idempotence automatically.

### Key Construction with Prefix

```mermaid
graph LR
    Input["remote_key<br/>'server/mods/mod.jar'"]
    Prefix["bucket_prefix<br/>'minecraft/prod/'"]
    Check{prefix.is_empty?}

    Input --> Check
    Prefix --> Check

    Check -->|Yes| Direct["remote_key<br/>'server/mods/mod.jar'"]
    Check -->|No| Concat["prefix + '/' + remote_key<br/>'minecraft/prod/server/mods/mod.jar'"]
```

**Prefix Advantages:**
- Logical organization in bucket
- Environment separation (prod/dev)
- Multi-tenant support
- Facilitates IAM policies

### Public URL Generation

```mermaid
graph LR
    RemoteKey["remote_key<br/>'server/mods/mod.jar'"] --> BuildKey[build_key]
    Prefix["bucket_prefix"] --> BuildKey
    BuildKey --> FullKey["full_key<br/>'minecraft/prod/server/mods/mod.jar'"]

    PublicURL["public_url<br/>'https://cdn.example.com'"] --> Concat[Concatenation]
    FullKey --> Concat
    Concat --> Result["Complete URL<br/>'https://cdn.example.com/minecraft/prod/server/mods/mod.jar'"]
```

## Configuration by Provider

### Cloudflare R2

```toml
[storage]
type = "s3"
endpoint_url = "https://[account-id].r2.cloudflarestorage.com"
region = "auto"
access_key_id = "your-access-key"
secret_access_key = "your-secret-key"
bucket_name = "minecraft-files"
public_url = "https://cdn.example.com"
bucket_prefix = "servers/"
```

**Specifics:**
- Region always `auto`
- No egress fees
- Native integration with Cloudflare CDN
- 100% compatible API

### AWS S3

```toml
[storage]
type = "s3"
endpoint_url = "https://s3.us-east-1.amazonaws.com"
region = "us-east-1"
access_key_id = "AKIAIOSFODNN7EXAMPLE"
secret_access_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
bucket_name = "my-minecraft-bucket"
public_url = "https://my-minecraft-bucket.s3.amazonaws.com"
bucket_prefix = ""
```

**Specifics:**
- Region must match bucket
- Can use CloudFront as CDN
- Full S3 feature support

### MinIO (self-hosted)

```toml
[storage]
type = "s3"
endpoint_url = "http://minio.local:9000"
region = "us-east-1"
access_key_id = "minioadmin"
secret_access_key = "minioadmin"
bucket_name = "minecraft"
public_url = "http://cdn.local"
bucket_prefix = "prod/"
```

**Specifics:**
- HTTP possible (dev only)
- Region can be arbitrary
- Default credentials: minioadmin/minioadmin
- Optimal local performance

## Optimizations

### File Streaming

For large files, the SDK supports streaming:

```mermaid
sequenceDiagram
    participant S3B as S3Backend
    participant File as File Handle
    participant Stream as ByteStream
    participant S3 as S3 API

    S3B->>File: Open file
    loop 8KB chunks
        File->>Stream: Read chunk
        Stream->>S3: Send chunk
    end
    File->>File: EOF
    Stream->>S3: Finalize
    S3-->>S3B: Upload complete
```

Currently, implementation reads complete file. To support streaming:
- Use `ByteStream::from_path` instead of `ByteStream::from`
- Avoid `tokio::fs::read` which loads everything in memory
- Allow upload of files > available RAM

### Parallel Uploads

```mermaid
graph TB
    Files[100 files to upload] --> Batch1[Batch 1: 10 files]
    Files --> Batch2[Batch 2: 10 files]
    Files --> Batch3[Batch 3: 10 files]
    Files --> BatchN[Batch N: 10 files]

    Batch1 --> S3[S3 Upload]
    Batch2 --> S3
    Batch3 --> S3
    BatchN --> S3

    S3 --> Results[Collect results]
```

CacheManager parallelizes uploads via tokio::spawn to maximize throughput.

### Connection Pooling

AWS SDK automatically maintains:
- HTTP/2 connection pool
- TCP connection reuse
- Keep-alive to reduce latency

## Security

### Credentials

**Storage:**
- Never commit credentials to Git
- Use environment variables or secret files
- Regular rotation of access keys

**Minimal Permissions (IAM):**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:DeleteObject"
      ],
      "Resource": "arn:aws:s3:::bucket-name/*"
    }
  ]
}
```

### Transport

- Always use HTTPS in production
- SSL certificate verification
- TLS 1.2+ minimum

### Validation

- Backend doesn't validate file contents
- Scanner's responsibility to verify files
- No protection against malicious file uploads

## Monitoring and Debugging

### Logs

Backend emits structured logs:

```rust
tracing::info!(
    bucket = %self.bucket_name,
    key = %key,
    "Uploading to S3"
);

tracing::info!(
    url = %url,
    "Upload complete"
);
```

### Metrics

Important metrics to monitor:
- Upload success rate
- Average operation latency
- Uploaded file sizes
- Bandwidth consumed
- Error rate by type

### Troubleshooting

**Upload fails with 403:**
- Verify credentials
- Verify IAM permissions
- Verify bucket name

**Upload fails with timeout:**
- Verify network connectivity
- Increase SDK timeout
- Verify file size

**Incorrect URLs:**
- Verify `public_url` in config
- Verify `bucket_prefix`
- Test URL manually

## Performance

### Benchmarks

**Upload (10MB file):**
- Cloudflare R2: ~200-500ms
- AWS S3 (us-east-1): ~300-800ms
- MinIO (local): ~50-150ms

**Delete:**
- All platforms: ~50-200ms

**Network Latency:**
- Main determining factor
- Use region close to servers
- CDN for global distribution

### Recommended Optimizations

1. **Compression:**
   - Compress files before upload if possible
   - Bandwidth and upload time savings

2. **Batch operations:**
   - Group small files
   - Parallelize uploads
   - Limit concurrency (10-20 simultaneous)

3. **CDN:**
   - Use CloudFront, Cloudflare, or Fastly
   - Cache files closer to clients
   - Reduce load on S3

4. **Lifecycle policies:**
   - Automatically delete old versions
   - Archive infrequently accessed files
   - Reduce storage costs
